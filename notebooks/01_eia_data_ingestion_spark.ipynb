{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbbb63ea",
   "metadata": {},
   "source": [
    "# EIA-860 Generator Data Ingestion (PySpark)\n",
    "\n",
    "This notebook:\n",
    "- Pulls EIA-860 operating generator capacity data via API (last 15 years)\n",
    "- Uses PySpark for data processing (local mode for now, scales to cluster later)\n",
    "- Saves cleaned data to Parquet format\n",
    "- Handles pagination, retries, and resume capability\n",
    "\n",
    "**Requirements:**\n",
    "- `.env` file with `EIA_API_KEY=YOUR_KEY`\n",
    "- Java 11+ installed (we're using Temurin 17)\n",
    "- PySpark installed (`pip install pyspark`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e7e9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded: True\n",
      "Java version check: Not set - using system default\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "EIA_API_KEY = os.getenv(\"EIA_API_KEY\")\n",
    "\n",
    "print(f\"API key loaded: {bool(EIA_API_KEY)}\")\n",
    "print(f\"Java version check: {os.environ.get('JAVA_HOME', 'Not set - using system default')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b95c5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.1.1\n",
      "Spark master: local[*]\n"
     ]
    }
   ],
   "source": [
    "# Tell Spark where Python is\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Initialize Spark session (local mode)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EIA_Generator_Ingestion\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.RawLocalFileSystem\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28de2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "RAW_DATA_DIR = Path(\"../data/raw\")\n",
    "PROCESSED_DATA_DIR = Path(\"../data/processed\")\n",
    "\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_PARQUET_PATH = RAW_DATA_DIR / \"eia_generators_raw.parquet\"\n",
    "PROCESSED_PARQUET_PATH = PROCESSED_DATA_DIR / \"eia_generators_cleaned.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f46c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EIA API configuration\n",
    "BASE_URL = \"https://api.eia.gov/v2/electricity/operating-generator-capacity/data/\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"api_key\": EIA_API_KEY,\n",
    "    \"frequency\": \"monthly\",\n",
    "    \n",
    "    # Data columns\n",
    "    \"data[0]\": \"county\",\n",
    "    \"data[1]\": \"latitude\",\n",
    "    \"data[2]\": \"longitude\",\n",
    "    \"data[3]\": \"nameplate-capacity-mw\",\n",
    "    \"data[4]\": \"net-summer-capacity-mw\",\n",
    "    \"data[5]\": \"net-winter-capacity-mw\",\n",
    "    \"data[6]\": \"operating-year-month\",\n",
    "    \"data[7]\": \"planned-derate-summer-cap-mw\",\n",
    "    \"data[8]\": \"planned-derate-year-month\",\n",
    "    \"data[9]\": \"planned-retirement-year-month\",\n",
    "    \"data[10]\": \"planned-uprate-summer-cap-mw\",\n",
    "    \"data[11]\": \"planned-uprate-year-month\",\n",
    "    \n",
    "    # Sort and filter\n",
    "    \"sort[0][column]\": \"period\",\n",
    "    \"sort[0][direction]\": \"desc\",\n",
    "    \"start\": \"2011-01\",\n",
    "    \"end\": \"2025-12\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca461da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the data (load everything as strings first)\n",
    "schema = StructType([\n",
    "    StructField(\"period\", StringType(), True),\n",
    "    StructField(\"stateDescription\", StringType(), True),\n",
    "    StructField(\"balancingAuthorityCode\", StringType(), True),\n",
    "    StructField(\"plantCode\", StringType(), True),\n",
    "    StructField(\"plantName\", StringType(), True),\n",
    "    StructField(\"generatorId\", StringType(), True),\n",
    "    StructField(\"sector\", StringType(), True),\n",
    "    StructField(\"technology\", StringType(), True),\n",
    "    StructField(\"energySourceCode\", StringType(), True),\n",
    "    StructField(\"primeMoverCode\", StringType(), True),\n",
    "    StructField(\"county\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),  # ‚Üê Changed to StringType\n",
    "    StructField(\"longitude\", StringType(), True),  # ‚Üê Changed to StringType\n",
    "    StructField(\"nameplate-capacity-mw\", StringType(), True),  # ‚Üê Changed to StringType\n",
    "    StructField(\"net-summer-capacity-mw\", StringType(), True),  # ‚Üê Changed to StringType\n",
    "    StructField(\"net-winter-capacity-mw\", StringType(), True),  # ‚Üê Changed to StringType\n",
    "    StructField(\"operating-year-month\", StringType(), True),\n",
    "    StructField(\"planned-derate-summer-cap-mw\", StringType(), True),  # ‚Üê Changed to StringType\n",
    "    StructField(\"planned-derate-year-month\", StringType(), True),\n",
    "    StructField(\"planned-retirement-year-month\", StringType(), True),\n",
    "    StructField(\"planned-uprate-summer-cap-mw\", StringType(), True),  # ‚Üê Changed to StringType\n",
    "    StructField(\"planned-uprate-year-month\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5094ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_with_retries(url, params, max_retries=5, backoff_seconds=5):\n",
    "    \"\"\"\n",
    "    Fetch a single page from the EIA API with exponential backoff retry logic.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code == 503:\n",
    "                wait_time = backoff_seconds * (attempt + 1)\n",
    "                print(f\"Server error 503. Retry {attempt + 1}/{max_retries} after {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Unexpected status code: {response.status_code}\")\n",
    "                response.raise_for_status()\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            wait_time = backoff_seconds * (attempt + 1)\n",
    "            print(f\"Request timeout. Retry {attempt + 1}/{max_retries} after {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            time.sleep(backoff_seconds * (attempt + 1))\n",
    "    \n",
    "    raise Exception(f\"Failed to fetch data after {max_retries} retries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd670f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_eia_data_simple():\n",
    "    \"\"\"\n",
    "    Simpler ingestion: fetch data with pandas, save to parquet.\n",
    "    No Spark workers needed during ingestion.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    length = 5000\n",
    "    offset = 0\n",
    "    total = None\n",
    "    all_batches = []\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Check if we should resume\n",
    "    if RAW_PARQUET_PATH.exists():\n",
    "        existing = pd.read_parquet(RAW_PARQUET_PATH)\n",
    "        rows_loaded = len(existing)\n",
    "        print(f\"Found existing data: {rows_loaded:,} rows\")\n",
    "        offset = (rows_loaded // length) * length\n",
    "        batch_count = offset // length\n",
    "        print(f\"Resuming from offset: {offset:,} (batch {batch_count + 1})\")\n",
    "    else:\n",
    "        print(\"Starting fresh ingestion with pandas...\")\n",
    "    \n",
    "    while True:\n",
    "        params = PARAMS.copy()\n",
    "        params[\"offset\"] = offset\n",
    "        params[\"length\"] = length\n",
    "        \n",
    "        try:\n",
    "            response = fetch_page_with_retries(BASE_URL, params)\n",
    "            data_json = response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"Fatal error at offset {offset}: {e}\")\n",
    "            print(\"Saving progress...\")\n",
    "            break\n",
    "        \n",
    "        records = data_json.get(\"response\", {}).get(\"data\", [])\n",
    "        \n",
    "        if total is None:\n",
    "            total = int(data_json[\"response\"][\"total\"])\n",
    "            estimated_batches = (total + length - 1) // length\n",
    "            print(f\"Total records available: {total:,}\")\n",
    "            print(f\"Estimated batches needed: {estimated_batches:,}\")\n",
    "        \n",
    "        if not records:\n",
    "            print(\"No more records.\")\n",
    "            break\n",
    "        \n",
    "        batch_df = pd.DataFrame(records)\n",
    "        all_batches.append(batch_df)\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Progress display\n",
    "        fetched_so_far = offset + len(records)\n",
    "        pct_complete = (fetched_so_far / total * 100) if total else 0\n",
    "        print(f\"Batch {batch_count:,} | Fetched {len(records):,} rows at offset {offset:,} | \"\n",
    "              f\"Total: {fetched_so_far:,}/{total:,} ({pct_complete:.1f}%)\")\n",
    "        \n",
    "        offset += length\n",
    "        \n",
    "        # Save checkpoint every 100k rows\n",
    "        if offset % 100000 == 0:\n",
    "            print(f\"üîÑ Checkpoint save at {offset:,} rows...\")\n",
    "            combined = pd.concat(all_batches, ignore_index=True)\n",
    "            \n",
    "            if RAW_PARQUET_PATH.exists():\n",
    "                existing = pd.read_parquet(RAW_PARQUET_PATH)\n",
    "                combined = pd.concat([existing, combined], ignore_index=True)\n",
    "            \n",
    "            combined.to_parquet(RAW_PARQUET_PATH, index=False)\n",
    "            all_batches = []  # Clear memory\n",
    "            print(f\"‚úÖ Checkpoint saved ({len(combined):,} total rows)\")\n",
    "        \n",
    "        if offset >= total:\n",
    "            print(\"Reached end of dataset.\")\n",
    "            break\n",
    "    \n",
    "    # Final save\n",
    "    if all_batches:\n",
    "        print(\"üíæ Saving final batch...\")\n",
    "        new_data = pd.concat(all_batches, ignore_index=True)\n",
    "        \n",
    "        if RAW_PARQUET_PATH.exists():\n",
    "            existing = pd.read_parquet(RAW_PARQUET_PATH)\n",
    "            combined = pd.concat([existing, new_data], ignore_index=True)\n",
    "        else:\n",
    "            combined = new_data\n",
    "        \n",
    "        combined.to_parquet(RAW_PARQUET_PATH, index=False)\n",
    "        print(f\"‚úÖ Done! Total rows: {len(combined):,}\")\n",
    "        print(f\"üìä Total batches processed: {batch_count:,}\")\n",
    "    else:\n",
    "        combined = pd.read_parquet(RAW_PARQUET_PATH)\n",
    "        print(f\"‚úÖ Ingestion complete! Total rows: {len(combined):,}\")\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "491809a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing data: 3,972,719 rows\n",
      "Resuming from offset: 3,970,000 (batch 795)\n",
      "Total records available: 3,972,719\n",
      "Estimated batches needed: 795\n",
      "Batch 795 | Fetched 2,719 rows at offset 3,970,000 | Total: 3,972,719/3,972,719 (100.0%)\n",
      "Reached end of dataset.\n",
      "üíæ Saving final batch...\n",
      "‚úÖ Done! Total rows: 3,975,438\n",
      "üìä Total batches processed: 795\n"
     ]
    }
   ],
   "source": [
    "# Run pandas ingestion\n",
    "df_pandas = ingest_eia_data_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "192193b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the raw data (if resuming from a previous run)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_raw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m RAW_PARQUET_PATH\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m      3\u001b[0m     df_raw \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;28mstr\u001b[39m(RAW_PARQUET_PATH))\n\u001b[0;32m      4\u001b[0m     df_raw\u001b[38;5;241m.\u001b[39mcache()  \u001b[38;5;66;03m# ‚Üê ADD THIS\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the raw data (if resuming from a previous run)\n",
    "if df_raw is None and RAW_PARQUET_PATH.exists():\n",
    "    df_raw = spark.read.parquet(str(RAW_PARQUET_PATH))\n",
    "    df_raw.cache()  # ‚Üê ADD THIS\n",
    "    print(f\"Cached raw data in memory\")\n",
    "\n",
    "# Basic stats\n",
    "print(f\"Total rows: {df_raw.count():,}\")  # This triggers the cache\n",
    "print(f\"\\nSchema:\")\n",
    "df_raw.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
